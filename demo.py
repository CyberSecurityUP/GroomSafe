#!/usr/bin/env python3
"""
GROOMSAFE - DemonstraÃ§Ã£o RÃ¡pida
Analisa as 4 conversas de exemplo
"""

import json
import sys
from pathlib import Path

sys.path.insert(0, "groomsafe")

from core.data_models import Conversation
from core.risk_scoring import RiskScoringEngine

datasets = [
    ("BAIXO RISCO", "low_risk_conversation.json", "InteraÃ§Ã£o educacional normal"),
    ("RISCO MODERADO", "moderate_risk_conversation.json", "Alguns padrÃµes preocupantes"),
    ("ALTO RISCO", "high_risk_conversation.json", "MÃºltiplos indicadores de risco"),
    ("RISCO CRÃTICO", "critical_risk_conversation.json", "EscalaÃ§Ã£o crÃ­tica - aÃ§Ã£o urgente"),
]

engine = RiskScoringEngine()

print("=" * 80)
print("ğŸ›¡ï¸  GROOMSAFE - DEMONSTRAÃ‡ÃƒO DE ANÃLISE DE RISCO")
print("=" * 80)
print()

for nome, arquivo, descricao in datasets:
    caminho = Path(f"groomsafe/data/synthetic/{arquivo}")

    with open(caminho) as f:
        conversa = Conversation(**json.load(f))

    avaliacao = engine.assess_risk(conversa)

    print(f"{'â”€' * 80}")
    print(f"ğŸ“Š {nome}")
    print(f"{'â”€' * 80}")
    print(f"DescriÃ§Ã£o: {descricao}")
    print(f"Mensagens: {len(conversa.messages)}")
    print()
    print(f"  PontuaÃ§Ã£o de Risco:  {avaliacao.grooming_risk_score:>6.1f}/100")
    print(f"  NÃ­vel:               {avaliacao.risk_level.value.upper():>12}")
    print(f"  ConfianÃ§a:           {avaliacao.confidence_level:>11.1%}")
    print(f"  EstÃ¡gio:             {avaliacao.current_stage.value.replace('_', ' ').title()}")
    print(f"  RevisÃ£o Humana:      {'SIM âš ï¸ ' if avaliacao.requires_human_review else 'NÃ£o'}")
    print()

print("=" * 80)
print("âœ… AnÃ¡lise Completa!")
print("=" * 80)
print()
print("ğŸ“– Para mais detalhes, execute:")
print("   python3 direct_test.py          # Teste detalhado")
print("   python3 examples/example_usage.py  # Exemplos completos")
print()
print("ğŸŒ API Server: http://localhost:8090/docs")
print()
